---
title: "Лабораторная №1. Выявление характерной лексики в описаниях российских ресторанных бизнесов в зависимости от их ценовой категории"
author: "Виктория Болотова"
date: "17 10 2021"
output: 
    html_document:
      theme: cosmo
      code_folding: show
      toc: true
      toc_float: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Аналитическая задача, характеристика данных 

Здравствуйте! :) 

В этой лабораторной работе я использую данные с Kaggle примерно о 2740 ритейлеров, работающих в России (https://www.kaggle.com/pavelkunitsyn/russian-retail). Первоначальные данные включают в себя информацию о компаниях с различными специализациями бизнеса, **но я решила сфокусироваться на данных компаний, представляющих собой кафе/рестораны/столовые/кофейни/пабы**. Страна происхождения ресторанных бизнесов варьируется, преимущественно из России.

В этих данных есть привлекающая внимание колонка - **"description"**, в которой содержится *всестороннее описание бизнеса*. Я проверила, что описание бизнесов точь-в-точь совпадает с тем, что у самих бизнесов написано на их сайтах, то есть мы можем доверять этим данным. 

Содержание колонки "desciption" у Додо Пиццы:

*"Обычно люди приходят в Додо Пиццу, чтобы просто поесть. Наши промоутеры раздают листовки про кусочек пиццы за двадцать рублей или ещё что-то выгодное. Мы делаем это как первый шаг, чтобы познакомиться. Но для нас Додо — не только пицца. Это и пицца тоже, но в первую очередь это большое дело, которое вдохновляет нас, заставляет каждое утро просыпаться и с интересом продолжать работу. В чём же наш интерес? Сейчас расскажем.Почему мы так хотим познакомиться? Потому что дальше пицца делает всё сама. Люди видят, что она вкусная, и возвращаются снова. Нам главное первый раз это показать. Вообще пицца — очень простая штука, её сложно испортить. Достаточно хороших ингредиентов и правильного теста. Это конструктор, если детали качественные, то и результат будет в порядке. Вот они, наши детали: Кто угодно может сделать вкусную пиццу. А шеф-повар итальянского ресторана сделает и вовсе шедевр. Он молодец. Но представьте, что вам нужно сделать вкусную пиццу в сотнях пиццерий, за сотни километров друг от друга. Это наш случай, у нас их вон уже сколько: Пицца должна быть вкусной и везде одинаковой. Пиццерии должны быть одинаково уютными, кассиры неизменно приветливыми, а курьеры — расторопными. И это как раз сложно. Но мы умеем!"*

Именно такой текст размещен на офицальном сайте Додо. 


**Мой исследовательский интерес** к этим данным заключен в желании проанализировать, как **характерная лексика описания бизнесов** (как бизнесы сами себя представляют для потенциальных клиентов) **меняется в зависимости от ценновой категории бизнеса** (описательный график будет чуть позже). Для меня как социолога эта задача интересна, поскольку бизнесы в описаниях самих себя ставят **цель найти отклик от своей целевой аудитории**, а не просто презентовать себя. А поскольку российское общество достаточно сильно стратицифированно, то бизнесы, в зависимости от своей ценовой категории, стараются убедить свою целевую аудиторию, что они **смогут удовлетворить их нужды, пожелания**. В свою очередь, потребительское поведение и желания людей на самом деле значительно **определяются их социальным классом**, как писал Пьер Бурдье. 

**Потребление** является одной из практик, с помощью которых **люди проявляют принадлежность к своему социальному классу** или к классу, к которому они хотели бы принадлежать. Таким образом, я предполагаю, что мы с Вами увидим, как **рестораны и кафе "продают" опыт такой принадлежности** - возникает **"экономика впечатлений"** (Joseph Pine, James Gilmore 1998).

* Размер всего корпуса после препроцессинга: 31765 слов.

# Препроцессинг данных 

## Загружаем необходимые библиотеки и считываем данные

```{r}
library(readr)
library(dplyr)
library(ggplot2)
library(tidytext)
library(stringr)
library(tidyr)
library(tidylo) # для подсчёта weighted log-odds
retail <- read_csv("russian_retail.csv")
```

## Отфильтровываем данные и отбираем нужные переменные
```{r}
cafe <- retail %>% 
  filter(retail$domain == 'Кафе, ресторан') %>%
  select(name, price_category, description) 
nrow(cafe)
```

**Удаляем пропущенные значения**
```{r}
cafe <- na.omit(cafe)
nrow(cafe)
glimpse(cafe) #посмотрим на наши данные
```

Количество строк уменьшилось на единицу - то есть мы потеряли информацию только об одном кафе/ресторане.

**Визуализация** 

Давайте посмотрим на процентное отношения количества ресторанных бизнесов в зависимости от ценовой категории:


```{r}
ggplot() + geom_bar(data = cafe, fill = "skyblue", aes(x = as.factor(price_category), y=..count../sum(..count..)*100)) + labs(y="Количество бизнесов, %", x="Ценовые категории", title = "Ценовые категории кафе/ресторанов") + coord_flip() + theme_classic()
```

Больше всего представлен ресторанный бизнес средней ценовой категории. Нашим фокусным корпусом будут слова из описаний бизнесов, у которых ценовая категория:

* 'Средний; Выше среднего; Люкс / Премиум'
* 'Выше среднего'
* 'Выше среднего; Люкс / Премиум'
* 'Люкс / Премиум'
* 'Средний; Выше среднего'


## Чистка и нормализация текста


В описаниях бизнесов содержатся много цифр, поэтому давайте отчистим текст от них, так как важной информации они не представляют для данной аналитической задачи. 

**Избавляемся от цифр**

Избавляемся от цифр и сохраняем отчищенные от цифр описания в новую колонку `clean_desciption`

```{r}
cafe$clean_desciption <- str_replace_all(cafe$description, "[0-9]+", " ")
glimpse(cafe)
```

Лемматизация на мой взгляд необходима и её нужно провести до токенизации. 


**Лемматизация**

```{r}
cafe$lem <- system2("mystem", c("-d", "-l", "-c"), input = cafe$clean_desciption, stdout = TRUE)
glimpse(cafe)
```

Это лабороторная работа будет базироваться на основе текстового анализа описаний *342* бизнесов, специализирующехся в сфере ресторанов, кафе, пабов и т.д. 

Теперь можно смело приступать к токенизации текста на слова. 

**Токенизация**

```{r}
cafe_tokens <- cafe %>% unnest_tokens(word, lem)
glimpse(cafe_tokens)
nrow(cafe_tokens)
```

На этом этапе объем всего корпуса: 38405 слов

**Ограничение лексикона**

Отбросим все леммы, которые встречаются в описании только четырёх кафе/ресторанов. Я пробовала ограничивать слова, которые встречались меньшее количество раз (1, 2, 3), но последующий анализ дал мне понять, что 4 - оптимальный вариант. 

```{r}
stop_lem <- cafe_tokens %>%
    count(word, name) %>%
    group_by(word) %>%
    filter(n() < 4)
nrow(stop_lem)
```

Решение ограничить лексион привело к потере 5458 слов.

Давайте теперь посмотрим от чего мы избавились:

```{r}
stop_lem %>% arrange(desc(n)) %>% head(10)
```

- Из таблицы мы видим, что мы как раз избавились от слов, которые отражали название бизнеса, например:

  - слова *donuts* и *dankin* у компании *Dunkin Donuts*
  - слово *maki* у компании *MAKI MAKI*
  - слово *people* у компании *PEOPLE'S* 
  - и т.д.
  
- Почему я не ограничила слова, которые встречаются не более чем в одном описании бизнеса:
  - Дело в том, что, как я поняла в ходе вычитки описаний и поиска слов, некоторые отдельные бизнесы являются объединениями, поэтому их описания совпадают. Но в таких объединений не больше 4 кафе/ресторанов. 


Проводим антиджоин таблицы `cafe_tokens` с таблицей `stop_lem`:

```{r}
clean_tokens <- cafe_tokens %>% anti_join(stop_lem)
nrow(clean_tokens)
nrow(cafe_tokens)
```

Конечный объем корпуса: 31765 слов. Я решила не исключать стоп-слова, поскольку мне кажется это приведет к потере многих на самом деле важных для моей задачи слов, например, обращений (Вы и т.д.) или местоимений (мы). 


Основание выделение фокусного коруса - ценовая категория кафе/ресторана выше среднего и люкс/премиум


```{r}
corpus <- clean_tokens %>%
  mutate(corpus = ifelse(price_category %in% c('Выше среднего', 'Выше среднего; Люкс / Премиум', 	'Люкс / Премиум', 'Средний; Выше среднего', 'Средний; Выше среднего; Люкс / Премиум'), 'focus', 'reference'))
glimpse(corpus)
```

Для начала давайте посмотрим на простую частотность. 

```{r}
corpus_freq <- corpus %>%
    group_by(corpus) %>%
    count(word) %>%
    spread(corpus, n, fill=0) %>%
    arrange(desc(focus + reference))
corpus_freq
```

Все в порядке, можно приступать к следующему шагу.

# Выделение характерной лексики 

## Простое отношение нормализованной частотности слов (по simple maths Адама Килгариффа)


**При константе 1**

```{r}
cafe_ipm_n1 <- corpus_freq %>%
  mutate(f.ipm = (focus+1)/sum(focus+1)*10000, r.ipm = (reference+1)/sum(reference+1)*10000) %>%     mutate(sm = f.ipm/r.ipm) %>% arrange(desc(sm))
cafe_ipm_n1
```


**При константе 10**

```{r}
cafe_ipm_n10 <- corpus_freq %>%
  mutate(f.ipm = (focus+10)/sum(focus+10)*10000, 
         r.ipm = (reference+10)/sum(reference+10)*10000) %>% 
  mutate(sm = f.ipm/r.ipm) %>% arrange(desc(sm))
cafe_ipm_n10
```

При изменении константы на 10 список характерных слов не изменятся.


**При константе 100**

```{r}
cafe_ipm_n100 <- corpus_freq %>%
  mutate(f.ipm = (focus+100)/sum(focus+100)*10000, 
         r.ipm = (reference+100)/sum(reference+100)*10000) %>% 
  mutate(sm = f.ipm/r.ipm) %>% arrange(desc(sm))
cafe_ipm_n100
```

А вот изменение константы с 10 до 100 выводит слово `арабика` на 10 место, в то время как остальные слова остались на своих местах.

Выводы по частотной лексике, используя метод simple maths Адама Килгариффа:

* Мы видим целых три слова, связанных с тематикой кофе в топ-10: слова `зерно`, `обжарка`, `арабика` указывают на то, что кофейни или же кафе, специлизирующиеся на кофе, чаще ориентируются на посетителей, которые готовы платить больше, чем среднестатистический российский гражданин. 

* Слова `Лондон`, `Йорк` (Нью-Йорк), `Казахстан` тоже попавшие в топ-10 дают нам возможность предположить, что среди ресторанов и кафе,  у которых ценовая категория выше среднего, страна происхождения бизнеса чаще всего является Велибритания или США (проверила по поиску в данных). А Казахстан попал в топ-10 так как компании, ориентирующееся на ценовую категорию выше среднего, предпочитают открывать свои сети в этом государстве - о чем и сообщают нам в своих описаниях (тоже проверила). 

* Ресторанные бизнеса, чья ценовая категория выше среднего, чаще имееют кальянную зону. 

Что-ж, давайте посмотрим какие списки наиболее характерных слов мы получим, используя другие методы. 

## Отношение правдоподобия (Dunning log-likelihood)

Определим функцию для вычисления Dunning log-likelihood (G^2):

```{r}
g2 = function(a, b) {
  c = sum(a)
  d = sum(b)
  E1 = c * ((a + b) / (c + d))
  E2 = d * ((a + b) / (c + d))
  return(2*((a*log(a/E1+1e-7)) + (b*log(b/E2+1e-7))))
}
```


Применим функцию g2, определенную выше, для вычисления log-likelihood и отсортируем результаты по значению g2.

```{r}
cafe_ll <- corpus_freq %>%
    mutate(g2 = g2(focus, reference)) %>%
    arrange(desc(g2)) 

cafe_ll
```

Непонятное s вылезло на вершину нашего частотного списка, но остальные слова вполне себе подходящие для анализа. 

* Только одно слово из тематики кофе попало в топ-10 списка наиболее характерных слов по методу log-likelihood: `зерно` (`обжарка` на 11 месте)

* В топ-10 есть такие слова, как `американский`, `лондон`, `грузинский`, `казахстан`. Причину появления Казахстана и Лондона мы уже выяснили. `американский` появляется в контексте кухни ресторана и концепии всего бизнеса, как и `грузинский` (например, у таких заведений, как "Есть Хинкали & Пить Вино", "Грузинские каникулы" и т.д.)

* Стоит обратить внимание на слово `выпечка`. Оно встречается 0 раз в фокусном корпусе и целых 60 раз в референсном. В целом это ожидаемый результат, поскольку очевидно, что выпечка стоит недорого и при этом это сытно, поэтому навряд ли кафе и рестораны ценовой категорией выше среднего будут иметь у себя в меню много выпечки. Ещё одна причина почему мы увидели слово `выпечка` может заключаться в том, что люди с достатком чаще следят за фигурой и своим здоровьем, а мучные изделия предательски портят фигуру, поэтому рестораны и кафе высокой ценовой категорией, ориентируясь на свою целевую аудиторию, не включают в меню выпечку. Кроме того, люди разных социальных классов ходят в рестораны/кафе, удовлятворяю разные нужды и желания. Люди рабочего класса просто хотят поесть и выйти сытыми из заведения, в то время как люди среднего класса и выше хотят показать себя и получить новые эмоции, впечатления, поэтому их выпечкой не удовлетворить. 


Поговорим об используемом методе **log-likelihood**

* Этот метод привел нас к топ-10 списку наиболее характерных слов, которые довольно-таки часто встречаются как и в изучаемом (focus), так и в контрастном (reference) корпусе (11 раз, 16 раз, 10, 18 и даже 60). В то время как подход simple maths (при константе 100) привел нас к списку слов, в котором частотность ни одного слова в топ-10 не превышает 9. 

* Тут мы и столкнулись с проблемой **log-likelihood** - занижение степени различия по менее частотным словам. Таким образом, этот подход завысил степень различия по часто встречающемся словам, поэтому мы и обнаруживаем частотные слова на вершине наиболее характерной лексики. 

Давайте обратимся к другим методам.

## Pointwise mutual information (PMI)

Pointwise mutual information — PMI = log2(p(x,y)/p(x)p(y))

Или же: PMI = log2(p(x|y)/p(x)), где

* p(x|y) — вероятность встретить слово x в описании кафе/ресторанов, у которых ценовая категория выше среднего или люкс/премиум  p(x|кафе с высокой ценовой категорией).
* p(x) — вероятность встретить слова во всех описаниях всех кафе и ресторанов (в том числе и в описаниях кафе/ресторанов с ценовой категорией выше среднего/люкс/премиум).


Создадим колонки с двумя нужными нам для вычисления PMI вероятностями.


```{r}
cafe_p <- corpus_freq %>%
    mutate(p.x_y = focus/sum(focus), p.x = (focus+reference)/(sum(focus)+sum(reference)))
cafe_p
```

Создадим и вычислим колонку `focus.pmi` и отсортируем данные по ней.

```{r}
cafe_pmi <- cafe_p %>%
    mutate(focus.pmi = log2(p.x_y/p.x)) %>%
    arrange(desc(focus.pmi))
cafe_pmi
```

Результирующий список топ-10 наиболее характерной лексики для фокусного корпуса индентичен списку, полученному с помощью подхода simple maths, когда константа равнялась 1 и 10. Однако некоторые из следующих 10 слов (с 11 до 20) отличаются порядком от того, что нам предложил подход simple maths. 

Метод PMI славится и не очень приятными моментами, как завышение степени различия по редким словам. Например, слово `добавлять` встречается 3 раза в фокусном корусе и только 1 в референсном, но при этом оно занимает почётное бронзовое место в списке наиболее характерной лексики. При этот это слово во всех четырех употреблениях несет разный контекстный смысл (проверила).

Частотность следующих топ-слов с 11 до 20 не превышает 5. Например, слово `безупречный` встречается в обоих корпусах по два раза. Интересно ещё то, что абсолютно все слова с 11 до 20 встречаются в обоих корпусах индентичное количество раз. Дело в том, что PMI чувствителен не просто к редким словам, но высоко информативным относительно друг друга словам. Именно поэтому мы наблюдаем такую симметрию. 

Наш последний метод на сегодня - weighted log-odds.


## Weighted log-odds

```{r}
cafe_counts_lo <- corpus %>% count(corpus, word, sort=TRUE)
cafe_counts_lo
```

Добавим колонку со значением weighted_log_odds:

```{r}
cafe_lo <- cafe_counts_lo %>%
    bind_log_odds(corpus, word, n)
cafe_lo
```

Выберем наиболее характерные слова для описаний кафе/ресторанов, чья ценовая категория выше среднего или люкс/премиум - то есть наш фокусный корпус:


```{r}
cafe_lo_focus <- cafe_lo %>% filter(corpus == "focus") %>%
    arrange(-log_odds_weighted) 
cafe_lo_focus
```

Используя метод *weighted_log_odds* мы получили список слов, в которых обнаруживаются "стоп-слова":

* и
* в
* который
* мы

Для меня тут интересно местоимение `мы`. Наверное, мы можем предположить, что бизнесы ценовой категорией выше среднего, более склоны подчеркивать важность команды сотрудников кафе/ресторана и убеждать свою целевую аудиторию, что он (бизнес) позаботился о составе команды, которая напрямую влияет на качество времяпрепровождения посетителей в их заведении. 

Также, мы видим слова, которые уже появлялись в топ-10 наиболее характерной лексики, когда мы использовали другие методы:

* зерно (simple maths & log-likelihood & PMI)
* мороженое (simple maths - при константах 1 и 10 &  log-likelihood & PMI)
* американский (log-likelihood)
* мегаполис (simple maths & log-likelihood & PMI)


Новое не "стоп-слово" слово в топ-10:

* большой (контекст: большой ассортимент, большой угольный гриль, большой кирпичный мангал, самой большой сетью)


О самом методе *weighted_log_odds*:

* Мы видим наибольшее совпадение слов, полученных с помощью *weighted_log_odds*, с методом *log-likelihood*. Учитывая что подход log-likelihood более чувствителен к частотным словам, вполне ожидаемо получить такое совпадение, поскольку weighted_log_odds показывает еще более частотные слова, чем log-likelihood. 

* Действительно, *weighted_log_odds* выводит нам в топ-10 наиболее харатктерной лексики очень частотные слова - даже стоп-слова. Такой эффект возможен из-за того, что метод weighted log odds предпологает, что мы можем быть более уверенными в словах как в показателях специфичной лексики только когда слова встречаются очень много раз (как стоп-слова, например) и менее уверенными в словах, которых встречались только несколько раз.


Давайте посмотрим на референсный корпус:

```{r}
cafe_lo_reference <- cafe_lo %>% filter(corpus == "reference") %>%
    arrange(-log_odds_weighted)
cafe_lo_reference
```

* Мы видим четыре слова из топ-10, связанных с не очень правильным питанием: `выпечка`, `пирог`, `фуд` (фаст-фуд изначально - я проверила) и `изделия` (контекст: кондитерские изделия, хлебобулочные изделия)

* Частое употребление местоимения `ваш`

* Целых два однокоренных слова: `сотрудник`, `сотрудничество`

* Опять же в топ-10 наиболее характерной лексики попали слова с высокой частотностью (60, 42). Метрика log_odds_weighted уменьшается по мере уменьшения частотности слов. 


# Заключение о специфической лексике фокусного корпуса

* Кафе, специлизирующееся на кофе, представляют собой в среднем более высокую ценовую категорию бизнеса.

* Среди ресторанов и кафе, у которых ценовая категория выше среднего, страна происхождения бизнеса чаще всего является Велибритания или США. Кроме того, компании, ориентирующееся на ценовую категорию выше среднего, предпочитают расширять свой бизнес в Казахстане. 

* Рестораны и кафе, имеющие ценовую категорию выше среднего, чаще (по сравнению с ресторанами ценовой категорией средней и ниже средней) представляют собой грузинские и американские заведения (имеется в виду кухня и концепция места). 

* Рестораны, кафе, столовые и т.д., у которых ценовая категория **средняя или ниже среднего**, намного более вероятно будут предлагать посетителям различные хлебобулочные и кондитерские изделия, в сравнении с фокусным корпусом. 

* Рестораны и кафе, имеющие ценовую категорию выше среднего, часто употребляют местоимения `мы` в своих описаниях. Возможно, так они хотят подчеркнуть тщательный отбор людей в команду и в целом серьезное отношение к тем, кто будет "предоставлять сервис" посетителям, а отличный состав команды гарантирует приятное времяпрепровождения во всех аспектах. В то же время, заведения, ценовой категорией средней или ниже среднего, часто употребляют слово `сотрудник` в своих описаниях. Вероятно, бизнесы, в зависимости от их ценовой категории, по-разному возлагают обязанности, ответственность и при этом "приписывают" заслуги. Рестораны и кафе, имеющие ценовую категорию выше среднего, рассматривают всех сотрудников как единое целое - команду, поэтому используют `мы` в описаниях, в то время как бизнесы, представляющие референсный корпус, склоны возлагать обязанности на каждого отдельного сотрудника и, возможно, не экстраполировать результаты отдельных людей на всю команду. 

* Рестораны и кафе, имеющие ценовую категорию выше среднего, чаще готовы предложить мороженое своим посетителям, в сравнении с референсным корпусом. 

Вот и всё, надеюсь, Вам было интересно читать мою работу :) 



